{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MakeMoneywithTensorflow2.0.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "VGpvfdNL7sxM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Demo - Automated Trading App\n",
        "\n",
        "![alt text](https://i.imgur.com/eYJJla1.png)\n",
        "\n",
        "\n",
        "### Tutorial Outline\n",
        "1. Prerequisite videos\n",
        "2. Problems with Tensorflow 1.0\n",
        "3. Tensorflow 2.0 Features\n",
        "4. Build a stock prediction model w/ Tensorflow 2.0\n",
        "5. Tensorflow Serving Explained\n",
        "6. Simple Tensorflow Serving Demo\n",
        "7. Add User signup/login functionality\n",
        "8. Add payments functionality\n",
        "9. Deploying the app to Heroku\n",
        "10. Ways of Improving the App"
      ]
    },
    {
      "metadata": {
        "id": "acKCGJaUA43z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - Prerequisites for this video \n",
        "\n",
        "## 4 Steps\n",
        "\n",
        "### First, watch \n",
        "\n",
        "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/HhqhFbwiaig/0.jpg)](http://www.youtube.com/watch?v=HhqhFbwiaig)\n",
        "\n",
        "### Then, watch \n",
        "\n",
        "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/mrRfpiAwad0/0.jpg)](http://www.youtube.com/watch?v=mrRfpiAwad0)\n",
        "\n",
        "### Then, watch \n",
        "\n",
        "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/NzmoPqte4V4/0.jpg)](http://www.youtube.com/watch?v=NzmoPqte4V4)\n",
        "\n",
        "### Then, watch this playlist\n",
        "\n",
        "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/2FmcHiLCwTU/0.jpg)](http://www.youtube.com/watch?v=2FmcHiLCwTU)"
      ]
    },
    {
      "metadata": {
        "id": "6JnsvAiqCh0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 - Problems with Tensorflow 1.0\n",
        "\n",
        "## Problem # 1 - Static Computation Graphs\n",
        "\n",
        "#### Its called 'DataFlow'\n",
        "\n",
        "![alt text](https://www.researchgate.net/profile/Margaret_Burnett/publication/220391588/figure/fig2/AS:670720233308170@1536923573575/Dataflow-programming-in-Prograph-Here-the-programmer-is-using-the-low-level-primitive.png\n",
        ")\n",
        "- Its modeled after a common programming paradigm called Dataflow\n",
        "- In a dataflow graph, the nodes represent units of computation, and the edges represent the data consumed or produced by a computation.\n",
        "- For example, in a TensorFlow graph, the tf.matmul operation would correspond to a single node with two incoming edges (the matrices to be multiplied) and one outgoing edge (the result of the multiplication).\n",
        "\n",
        "#### Why Use DataFlow?\n",
        "\n",
        "1. Parallelism - easier to execute operations in parallel\n",
        "2. Distributed Execution - easier to partition the graph\n",
        "3. Compilation - XLA Compiler generates faster code using a graph structure\n",
        "4. Portability - Language independnet graph. \n",
        "\n",
        "#### Tensorflow 1.0 Steps\n",
        "\n",
        "1. Import the data, normalize it, or create data input pipeline.\n",
        "2. Define an algorithm — Define variables, structure of the algo, loss function, optimization technique, etc. Tensorflow creates static computational graphs for this.\n",
        "3. Feed the data through this computation graph, compute loss from loss function and and update the weights (variables) by backpropagating the error.\n",
        "4. Stop when you reach some stopping criteria.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/0*uvXAUUtje1B01o_s.png)"
      ]
    },
    {
      "metadata": {
        "id": "7G5Klsy2FYt6",
        "colab_type": "code",
        "outputId": "f8126cc0-a945-439f-a848-f624675afe8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Basic constant operations\n",
        "# The value returned by the constructor represents the output\n",
        "# of the Constant op.\n",
        "a = tf.constant(2)\n",
        "b = tf.constant(3)\n",
        "c = a + b\n",
        "d = a * b\n",
        "\n",
        "# Launch the default graph.\n",
        "with tf.Session() as sess:\n",
        "    print(\"a=2, b=3\")\n",
        "    print(\"Addition with constants: %i\" % sess.run(c))\n",
        "    print(\"Multiplication with constants: %i\" % sess.run(d))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a=2, b=3\n",
            "Addition with constants: 5\n",
            "Multiplication with constants: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wC2x7yGBHhPy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem #2 - Verbosity (high learning curve)\n",
        "\n",
        "- Variables, placeholders, servables, tensorboard, sessions, computation graphs, hyperparameter values,  formatting conventions, SO MUCH TO LEARN\n",
        "- And we haven't even begun to talk about machine learning theory"
      ]
    },
    {
      "metadata": {
        "id": "bcAaHIyVLpGf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " ## An example of verbosity!! Randomly taken from https://github.com/carpedm20/DCGAN-tensorflow/blob/master/model.py \n",
        " ## Yo, WTF is going on LOL (i mean tbh i understand, but still its too messy)\n",
        "  \n",
        "  def sigmoid_cross_entropy_with_logits(x, y):\n",
        "      try:\n",
        "        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)\n",
        "      except:\n",
        "        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y)\n",
        "\n",
        "    self.d_loss_real = tf.reduce_mean(\n",
        "      sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))\n",
        "    self.d_loss_fake = tf.reduce_mean(\n",
        "      sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))\n",
        "    self.g_loss = tf.reduce_mean(\n",
        "      sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_)))\n",
        "\n",
        "    self.d_loss_real_sum = scalar_summary(\"d_loss_real\", self.d_loss_real)\n",
        "    self.d_loss_fake_sum = scalar_summary(\"d_loss_fake\", self.d_loss_fake)\n",
        "                          \n",
        "    self.d_loss = self.d_loss_real + self.d_loss_fake\n",
        "\n",
        "    self.g_loss_sum = scalar_summary(\"g_loss\", self.g_loss)\n",
        "    self.d_loss_sum = scalar_summary(\"d_loss\", self.d_loss)\n",
        "\n",
        "    t_vars = tf.trainable_variables()\n",
        "\n",
        "    self.d_vars = [var for var in t_vars if 'd_' in var.name]\n",
        "    self.g_vars = [var for var in t_vars if 'g_' in var.name]\n",
        "\n",
        "    self.saver = tf.train.Saver()\n",
        "\n",
        "  def train(self, config):\n",
        "    d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n",
        "              .minimize(self.d_loss, var_list=self.d_vars)\n",
        "    g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n",
        "              .minimize(self.g_loss, var_list=self.g_vars)\n",
        "    try:\n",
        "      tf.global_variables_initializer().run()\n",
        "    except:\n",
        "      tf.initialize_all_variables().run()\n",
        "\n",
        "    self.g_sum = merge_summary([self.z_sum, self.d__sum,\n",
        "      self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n",
        "    self.d_sum = merge_summary(\n",
        "        [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])\n",
        "    self.writer = SummaryWriter(\"./logs\", self.sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cw4FQGDKNLiZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem #3 - Messy APIs\n",
        "\n",
        "- So many new packages being added\n",
        "- Lots of deprecated APIs\n",
        "- Lots of renaming of existing APIs \n",
        "\n",
        "![alt text](https://i.imgur.com/4VTU9RB.png)\n",
        "\n",
        "![alt text](https://i.imgflip.com/2xvf09.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "Ov_UwmS3NReg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem #4 - Hard to Debug \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RE7Xw2QILpF6",
        "colab_type": "code",
        "outputId": "5ad09f21-bb31-4aaa-f479-9d3638edb904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x = tf.constant(0.)\n",
        "y = tf.log(x)  + 1. / x ## This line is the problem, but the output doesn't tell us that\n",
        "z = y + 1.\n",
        "\n",
        "with tf.Session() as sess: \n",
        "  print(z.eval())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G52ViqGuOAlB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 Tensorflow 2.0 Features\n",
        "\n",
        "### Feature #1 - Rapid prototyping (Eager Execution by Default)\n",
        "\n",
        "- Eager execution is an imperative programming paradigm that evaluates operations immediately, without building graphs!\n",
        "- Operations return concrete values instead of constructing a computational graph to run later.\n",
        "- This makes it easy to get started with TensorFlow and debug models, and it reduces boilerplate as well. \n",
        "- This allows for an intuitive interface—Structure your code naturally and use Python data structures. Quickly iterate on small models and small data.\n",
        "- No sessions or placeholders, instead pass data into functions as an argument\n",
        "\n",
        "![alt text](https://i.imgur.com/YUlhihi.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "a6u1jkMdQpzg",
        "colab_type": "code",
        "outputId": "dac7aff4-8608-4151-bb5a-61fc99584849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.executing_eagerly() \n",
        "\n",
        "x = [[2.]]\n",
        "m = tf.matmul(x, x)\n",
        "print(\"hello, {}\".format(m))\n",
        "\n",
        "a = tf.constant([[1, 2],\n",
        "                 [3, 4]])\n",
        "print(a)\n",
        "\n",
        "# Use NumPy values\n",
        "import numpy as np\n",
        "\n",
        "c = np.multiply(a, b)\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 79.9MB 343kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.2MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 11.4MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 419kB 19.4MB/s \n",
            "\u001b[?25hhello, Tensor(\"MatMul:0\", shape=(1, 1), dtype=float32)\n",
            "Tensor(\"Const_7:0\", shape=(2, 2), dtype=int32)\n",
            "Tensor(\"mul_3:0\", shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "veevU1dMTG9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature # 2 - Easier Debugging (Imperative programming style)\n",
        "\n",
        "- Call ops directly to inspect running models and test changes\n",
        "- Use standard Python debugging tools for immediate error reporting.\n"
      ]
    },
    {
      "metadata": {
        "id": "jLtH2bvBUJv3",
        "colab_type": "code",
        "outputId": "e7ad5f88-8734-405d-e09e-56b40c6f9701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#yeahhhhh debugging with print, boss status\n",
        "x = [[2.]]\n",
        "m = tf.matmul(x, x)\n",
        "print(m)\n",
        "z = tf.matmul(m, m)\n",
        "print(z)\n",
        "# The 1x1 matrix [[4.]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"MatMul_4:0\", shape=(1, 1), dtype=float32)\n",
            "Tensor(\"MatMul_5:0\", shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GXauAFg0UdH3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature #3 - Less verbose (tf.keras)\n",
        "\n",
        "- tf.keras is now the official high level API\n",
        "- Distributed training is simple (1 line of code to enable)\n",
        "- Deprecated APIs have been removed"
      ]
    },
    {
      "metadata": {
        "id": "R5sN_kfBUsaP",
        "colab_type": "code",
        "outputId": "866f7cbc-8f32-460c-adb7-35ca4de58fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "# This code downloads data and trains a model in like 45 seconds. Sick. \n",
        "\n",
        "#import TF\n",
        "import tensorflow as tf\n",
        "\n",
        "#enable distributed training, we'll later need to put wrap\n",
        "# our model + training code with 'with mirrored_strategy_scope()'\n",
        "# See https://www.tensorflow.org/guide/distribute_strategy\n",
        "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "#Import handwritten character labeled dataset.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "#split it into training and testing data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# build a model using the keras sequential API. So pretty <3 \n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile it with the adam (gradient descent) optimizer, pick a loss function\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Train it!\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "#Test it!\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 10s 162us/sample - loss: 0.2925 - acc: 0.9159\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 10s 160us/sample - loss: 0.1408 - acc: 0.9582\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 9s 156us/sample - loss: 0.1053 - acc: 0.9682\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 9s 156us/sample - loss: 0.0874 - acc: 0.9728\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 9s 157us/sample - loss: 0.0747 - acc: 0.9764\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0757 - acc: 0.9769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07565579186398536, 0.9769]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "Y-MtmfJ0XS0H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature #4 -  More granular control (Full lower level API)\n",
        "\n",
        "- New flexibilities: full low-level API, internal operations are accessible now (tf.raw_ops), and inheritable interfaces for variables, checkpoints, and layers\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/0*fJ5u2WE51Oz44dr_)"
      ]
    },
    {
      "metadata": {
        "id": "tbrF5CO1rxoT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature #5 - Backwards compatible with Tensorflow 1.0 \n",
        "\n",
        "- A simple script converts TF 1.0 code to TF 2.0 code "
      ]
    },
    {
      "metadata": {
        "id": "59n8bclbr8wC",
        "colab_type": "code",
        "outputId": "18ac41ab-5646-43c1-af97-cb48e5df730d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!tf_upgrade_v2 --infile text_generation.py text_generation_upgraded.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: tf_upgrade_v2 [-h] [--infile INPUT_FILE] [--outfile OUTPUT_FILE]\n",
            "                     [--intree INPUT_TREE] [--outtree OUTPUT_TREE]\n",
            "                     [--copyotherfiles COPY_OTHER_FILES] [--inplace]\n",
            "                     [--reportfile REPORT_FILENAME]\n",
            "tf_upgrade_v2: error: unrecognized arguments: text_generation_upgraded.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Iw1qcTBuqOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature #6 - Tensorboard is now available in Colab"
      ]
    },
    {
      "metadata": {
        "id": "h00PeY8Yu3wZ",
        "colab_type": "code",
        "outputId": "9042976d-a27a-46a4-ffd7-ac42ebfc0240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-05 23:00:35--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.232.181.106, 52.204.188.97, 34.226.180.131, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.232.181.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.28M  35.3MB/s    in 0.4s    \n",
            "\n",
            "2019-04-05 23:00:35 (35.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sv6HBJW4uz16",
        "colab_type": "code",
        "outputId": "3d65cb5f-f5ed-410f-b294-67da7af55e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "\n",
        "#HOLY SH** awesome\n",
        "\n",
        "TODO saturday \n",
        "8-9 Stock prediction model refined https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/keras/basic_regression.ipynb \n",
        "9-10 TF Serving Explained \n",
        "10-11 Modify Old App to use New Logo, add dropdown for industry, add 2 stock visualizations + balance view\n",
        "11-12 Create technical story (Demo Simple TF Serving, review its compononents, add user + payments + viz + balance + deploy functionality)\n",
        "12-1:30 Record\n",
        "1:30+ Meet Max\n",
        "  \n",
        " Sunday \n",
        "\n",
        "10-11 Stock Prediction\n",
        "11-12 TF Serving Explained\n",
        "12-1 Modify Old App\n",
        "1-2 Create Technical Story\n",
        "2-3 Record\n",
        "3-3:30 Chill\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://f1e87aa8.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FAwWPadwvPhc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 - Build a stock prediction model w/ Tensorflow 2.0\n",
        "\n",
        "https://colab.research.google.com/drive/1XBP0Zh8K4g_n0A2p1UlGFf3dij0EX_Kt "
      ]
    },
    {
      "metadata": {
        "id": "tBXBtbiavU-J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 - Tensorflow Serving Explained\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/0*-LMpEp7g_s9_plHF.jpg)\n",
        "\n",
        "- TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments.\n",
        "- TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs.\n",
        "- TensorFlow Serving provides out of the box integration with TensorFlow models, but can be easily extended to serve other types of models.\n",
        "\n",
        "## Wait, why use Serving instead of a regular web server framework like Flask or Django?\n",
        "\n",
        "\n",
        "- TensorFlow-Serving allows developers to integrate client requests and data with deep learning models served independently of client systems. \n",
        "- Benefits of this include clients being able to make inferences on data without actually having to install TensorFlow or even have any contact with the actual model, and the ability to serve multiple clients with one instance of a model.\n",
        "\n",
        "## OK, yes w could wrap a simple model in an API endpoint written in a Python framework like Flask, Falcon or similar, and voilá we have an API. But there are some really good reasons we don’t want to do it that way:\n",
        "\n",
        "\n",
        "## Reason #1 - TF Serving is faster\n",
        "\n",
        "- If your model(s) are complex and run slowly on CPU, you would want to run your models on more accelerated hardware (like GPUs). Your API-microservice(s), on the other hand, usually run fine on CPU and they’re often running in “everything agnostic” Docker containers. In that case you may want to keep those two kinds of services on different hardware.\n",
        "\n",
        "## Reason #2 - TF Serving is more space efficient\n",
        "- If you start messing up your neat Docker images with heavy TensorFlow models, they grow in every possible direction (CPU usage, memory usage, container image size, and so on). You don’t want that.\n",
        "\n",
        "## Reason #3 - Its going a version control system built in\n",
        "- Let's say your service uses multiple models written in different versions of TensorFlow. Using all those TensorFlow versions in your Python API at the same time is going to be a total mess.\n",
        "\n",
        "- You could of course wrap one model into one API. Then you would have one service per model and you can run different services on different hardware. Perfect! Except, this is what TensorFlow Serving ModelServer is doing for you. So don’t go wrap an API around your Python code (where you’ve probably imported the entire tf library, tf.contrib, opencv, pandas, numpy, …). TensorFlow Serving ModelServer does that for you.\n",
        "\n",
        "## More importantly, the TensorFlow team wrote TensorFlow Serving and the ModelServer for a reason. They are probably better than you when writing a high performance serving system. Use it!\n",
        "\n",
        "## Deploy a new version of your model and let tensorflow serving gracefully finish current requests while starting to serve new requests with the new model.\n",
        "\n",
        "## This separates concerns, data scientists can focus on building great models while Ops can focus on building highly resilient and scalable architectures that can serve those models.\n",
        "\n",
        "![alt text](https://www.tensorflow.org/tfx/serving/images/serving_architecture.svg)\n",
        "\n",
        "## 9 Concepts to understand in Tensorflow Serving\n",
        "\n",
        "## Concept 1 - Servables\n",
        "\n",
        "- Servables are the central abstraction in TensorFlow Serving. \n",
        "- Servables are the underlying objects that clients use to perform computation (for example, a lookup or inference).\n",
        "- A single Servable might include anything from a single shard of a lookup table to a single model to a tuple of inference models. \n",
        "\n",
        "## Concept 2 - Servable Versions\n",
        "\n",
        "- TensorFlow Serving can handle one or more versions of a servable over the lifetime of a single server instance. \n",
        "- This enables fresh algorithm configurations, weights, and other data to be loaded over time. \n",
        "\n",
        "## Concept 3 - Servable Streams \n",
        "\n",
        "- A servable stream is the sequence of versions of a servable, sorted by increasing version numbers.\n",
        "\n",
        "## Concept 4 - Models\n",
        "\n",
        "- TensorFlow Serving represents a model as one or more servables. \n",
        "- A machine-learned model may include one or more algorithms (including learned weights) and lookup or embedding tables.\n",
        "\n",
        "## Concept 5 - Loaders\n",
        "\n",
        "- Loaders manage a servable's life cycle. \n",
        "\n",
        "## Concept 6 - Sources\n",
        "\n",
        "- Sources are plugin modules that find and provide servables.\n",
        "-Each Source provides zero or more servable streams. \n",
        "- For each servable stream, a Source supplies one Loader instance for each version it makes available to be loaded. \n",
        "\n",
        "## Concept 7 - Aspired Versions\n",
        "\n",
        "- Aspired versions represent the set of servable versions that should be loaded and ready. \n",
        "- Sources communicate this set of servable versions for a single servable stream at a time. \n",
        "\n",
        "## Concept 8 - Managers\n",
        "\n",
        "- Managers handle the full lifecycle of Servables, including:\n",
        "\n",
        "loading Servables\n",
        "serving Servables\n",
        "unloading Servables\n",
        "\n",
        "## Concept 9 - Core\n",
        "\n",
        "- Using the standard TensorFlow Serving APis, TensorFlow Serving Core manages the following aspects of servables:\n",
        "\n",
        "lifecycle\n",
        "metrics\n",
        "\n",
        "## So the steps are\n",
        "\n",
        "1. We train a tensroflow model called transformer on some data and are using it for inference. Then we train it on newer data.\n",
        "2. The source plugin creates a loader for a specific version of this model. This loader has all the metadata necessary to load this servable computation graph, it points to it on disk.\n",
        "3. The source then notifies the dynamic manager of the aspired version (the newer model)\n",
        "4. The dynamic manager applies whatever version policy we write and decides to load the new version\n",
        "5. The dynamic manager tells the loader there isn't enough  memory! So the loader instantiates the TF graph with the new weights. \n",
        "6. A client requests the latest version of the model, the manager returns a handle to the new version of the servable.\n",
        "\n",
        "\n",
        "\n",
        "## BTW - Serving isn't an HTTP server, its a gRPC server \n",
        "\n",
        "\n",
        "- In gRPC a client application can directly call methods on a server application on a different machine as if it was a local object, making it easier for you to create distributed applications and services. \n",
        "- As in many RPC systems, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types.\n",
        "- The server side code is automatically remotely available in the client. \n",
        "- The gRPC client side has a stub that provides all the methods the server has. Of course, when the data travel over the wire, it’s no more magic than HTTP going on. But it’s the way you as a developer write the communication that is different!\n",
        "\n",
        "My previous company Twilio can help \n",
        "\n",
        "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/RoXT_Rkg8LA/0.jpg)](https://www.youtube.com/watch?v=RoXT_Rkg8LA)\n",
        "\n",
        "## BTW - oh they recently added HTTP support, cool \n",
        "\n",
        "https://www.tensorflow.org/tfx/serving/api_rest"
      ]
    },
    {
      "metadata": {
        "id": "AoyfhiC38eZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 6 - TF Serving Demo \n",
        "\n",
        "https://github.com/tobegit3hub/simple_tensorflow_serving"
      ]
    },
    {
      "metadata": {
        "id": "IxtrYv-P8MfH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Steps 7- 9\n",
        "\n",
        "Using https://github.com/tobegit3hub/simple_tensorflow_serving as a base we'll \n",
        "\n",
        "- Add User signup/login functionality\n",
        "- Add payments functionality\n",
        "- Deploying the app to Heroku"
      ]
    },
    {
      "metadata": {
        "id": "ooyetzC08PV3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 10 Ways of Improving the App\n",
        "\n",
        "- Ensemble our models\n",
        "- Better Design\n",
        "- Learn from user interaction \n",
        "- Sentiment Analysis for news\n",
        "- Try different models"
      ]
    }
  ]
}